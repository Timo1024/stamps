{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "import glob\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from matplotlib import colors\n",
    "from matplotlib import colorbar\n",
    "from matplotlib import patches\n",
    "from matplotlib import lines\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import ticker\n",
    "from matplotlib import transforms\n",
    "from matplotlib import font_manager\n",
    "from matplotlib import animation\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import rc\n",
    "from matplotlib import dates\n",
    "from PIL import Image, ImageEnhance, ImageFilter, ImageDraw\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Lambda, concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import tensorflow.keras.backend as K\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import contextlib\n",
    "import io\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAugmentation:\n",
    "    def __init__(self, image_path, output_dir='./images/augmented'):\n",
    "        self.image = Image.open(image_path)\n",
    "        self.base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        # self.output_dir = './images/augmented_images'\n",
    "        self.output_dir = os.path.join(output_dir, self.base_name)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def bezier_curve(self, p0, p1, p2, t):\n",
    "        \"\"\"Calculate a point on a quadratic Bezier curve.\"\"\"\n",
    "        return (\n",
    "            (1 - t) ** 2 * p0[0] + 2 * (1 - t) * t * p1[0] + t ** 2 * p2[0],\n",
    "            (1 - t) ** 2 * p0[1] + 2 * (1 - t) * t * p1[1] + t ** 2 * p2[1]\n",
    "        )\n",
    "\n",
    "    def draw_bezier_curve(self, draw, p0, p1, p2, color, thickness):\n",
    "        \"\"\"Draw a quadratic Bezier curve using line segments.\"\"\"\n",
    "        steps = 100  # Number of steps for approximation\n",
    "        prev_point = self.bezier_curve(p0, p1, p2, 0)\n",
    "        for i in range(1, steps + 1):\n",
    "            t = i / steps\n",
    "            current_point = self.bezier_curve(p0, p1, p2, t)\n",
    "            draw.line([prev_point, current_point], fill=color, width=thickness)\n",
    "            prev_point = current_point\n",
    "\n",
    "    def save_image(image, output_path):\n",
    "        # Save the modified image to the specified path\n",
    "        image.save(output_path)\n",
    "\n",
    "    def adjust_brightness(self, factor):\n",
    "        enhancer = ImageEnhance.Brightness(self.image)\n",
    "        out = enhancer.enhance(factor)\n",
    "        out.save(os.path.join(self.output_dir, f\"{self.base_name}_bright.jpg\"))\n",
    "\n",
    "    def adjust_contrast(self, factor):\n",
    "        enhancer = ImageEnhance.Contrast(self.image)\n",
    "        out = enhancer.enhance(factor)\n",
    "        out.save(os.path.join(self.output_dir, f\"{self.base_name}_contrast.jpg\"))\n",
    "\n",
    "    def adjust_saturation(self, factor):\n",
    "        enhancer = ImageEnhance.Color(self.image)\n",
    "        out = enhancer.enhance(factor)\n",
    "        out.save(os.path.join(self.output_dir, f\"{self.base_name}_saturated.jpg\"))\n",
    "\n",
    "    def add_gaussian_noise(self, mean=0, var=0.1):\n",
    "        img = np.array(self.image)\n",
    "        row, col, ch = img.shape\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "        noisy = img + gauss\n",
    "        noisy_image = np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "        Image.fromarray(noisy_image).save(os.path.join(self.output_dir, f\"{self.base_name}_noisy.jpg\"))\n",
    "\n",
    "    def blur_image(self, radius=2):\n",
    "        out = self.image.filter(ImageFilter.GaussianBlur(radius))\n",
    "        out.save(os.path.join(self.output_dir, f\"{self.base_name}_blurred.jpg\"))\n",
    "\n",
    "    def add_curved_lines(self, num_lines=5, thickness=3):\n",
    "        image = self.image.convert(\"RGBA\")\n",
    "        width, height = image.size\n",
    "        overlay = Image.new(\"RGBA\", (width, height), (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "        for _ in range(num_lines):\n",
    "            p0 = (random.randint(0, width), random.randint(0, height))\n",
    "            p1 = (random.randint(0, width), random.randint(0, height))\n",
    "            p2 = (random.randint(0, width), random.randint(0, height))\n",
    "            line_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255), random.randint(150, 255))\n",
    "            \n",
    "            # Draw the Bezier curve\n",
    "            self.draw_bezier_curve(draw, p0, p1, p2, line_color, thickness)\n",
    "\n",
    "        return Image.alpha_composite(image, overlay).convert(\"RGB\")\n",
    "\n",
    "    def add_straight_lines(self, num_lines=5, thickness=3):\n",
    "        image = self.image.convert(\"RGBA\")\n",
    "        width, height = image.size\n",
    "        overlay = Image.new(\"RGBA\", (width, height), (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "        for _ in range(num_lines):\n",
    "            x1, y1 = random.randint(0, width), random.randint(0, height)\n",
    "            x2, y2 = random.randint(0, width), random.randint(0, height)\n",
    "            line_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255), random.randint(150, 255))\n",
    "            draw.line((x1, y1, x2, y2), fill=line_color, width=thickness)\n",
    "\n",
    "        return Image.alpha_composite(image, overlay).convert(\"RGB\")\n",
    "\n",
    "    def add_lines(self, num_lines=5, thickness=3, ratio_curved=0.5, color=(0, 0, 0), count=0):\n",
    "        image = self.image.convert(\"RGBA\")\n",
    "        width, height = image.size\n",
    "        overlay = Image.new(\"RGBA\", (width, height), (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "        for _ in range(num_lines):\n",
    "\n",
    "            # random color (black with random alpha)\n",
    "            color_random_alpha = color + (random.randint(150, 255),)\n",
    "            # color_random_alpha = color\n",
    "            thickness_random = random.randint(1, thickness)\n",
    "\n",
    "            if random.random() < ratio_curved:\n",
    "                p0 = (random.randint(0, width), random.randint(0, height))\n",
    "                p1 = (random.randint(0, width), random.randint(0, height))\n",
    "                p2 = (random.randint(0, width), random.randint(0, height))\n",
    "                line_color = color_random_alpha\n",
    "                \n",
    "                # Draw the Bezier curve\n",
    "                self.draw_bezier_curve(draw, p0, p1, p2, line_color, thickness_random)\n",
    "            else:\n",
    "                x1, y1 = random.randint(0, width), random.randint(0, height)\n",
    "                x2, y2 = random.randint(0, width), random.randint(0, height)\n",
    "                line_color = color_random_alpha\n",
    "\n",
    "                draw.line((x1, y1, x2, y2), fill=line_color, width=thickness_random)\n",
    "\n",
    "        combined_image = Image.alpha_composite(image, overlay)\n",
    "        combined_image.convert(\"RGB\").save(os.path.join(self.output_dir, f\"{self.base_name}_lines_{count}.jpg\"))\n",
    "\n",
    "    def create_all_images(self):\n",
    "        self.adjust_brightness(1.5)\n",
    "        self.adjust_contrast(1.5)\n",
    "        self.adjust_saturation(1.5)\n",
    "        self.add_gaussian_noise(mean=0, var=30)\n",
    "        self.blur_image(radius=1)\n",
    "        for i in range(50):\n",
    "            amount_of_lines = random.randint(1, 20)\n",
    "            ratio_curved = random.uniform(0, 1)\n",
    "            self.add_lines(num_lines=amount_of_lines, thickness=3, ratio_curved=ratio_curved, color=(0, 0, 0), count=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create the invariants\n",
    "\n",
    "for now just for a subset of 1000 images (folders are images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy random 1000 images from ./images/original/ to ./images/original_test/\n",
    "def copy_images_to_test():\n",
    "    os.makedirs('./images/original_test', exist_ok=True)\n",
    "    image_files = glob.glob('./images/original/*.jpg')\n",
    "    random.shuffle(image_files)\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        if i < 1000:\n",
    "            shutil.copy(image_file, './images/original_test/')\n",
    "            # print from and to\n",
    "            # print(f\"{image_file} -> ./images/original_test/\")\n",
    "\n",
    "copy_images_to_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the invariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b945242d9d5449caeea1c79c75ffca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for all images in ./images/original_test/ create augmented images\n",
    "image_files = glob.glob('./images/original_test/*.jpg')\n",
    "for image_file in tqdm(image_files):\n",
    "    image_augmentation = ImageAugmentation(image_file, output_dir='./images/augmented_test')\n",
    "    image_augmentation.create_all_images()\n",
    "    # print(f\"Augmented images created for {image_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ee6cd0c43b4d109eb089a80aeffbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 triplets for training.\n"
     ]
    }
   ],
   "source": [
    "def prepare_triplets(image_folder_original, image_folder_augmented, num_triplets_per_anchor=10):\n",
    "    \"\"\"\n",
    "    Prepare multiple triplets for training. This function creates multiple triplets (anchor, positive, negative)\n",
    "    for each original image using various augmented images and different negative samples.\n",
    "    \n",
    "    :param image_folder_original: Folder containing original images.\n",
    "    :param image_folder_augmented: Folder containing augmented images in subfolders named after each original image.\n",
    "    :param num_triplets_per_anchor: Number of triplets to generate per original image.\n",
    "    :return: Arrays of anchor, positive, and negative images.\n",
    "    \"\"\"\n",
    "    anchor_images = []\n",
    "    positive_images = []\n",
    "    negative_images = []\n",
    "\n",
    "    # List all original images\n",
    "    original_images = [os.path.join(image_folder_original, f) for f in os.listdir(image_folder_original) if f.endswith('.jpg')]\n",
    "\n",
    "    # Create multiple triplets for each anchor\n",
    "    for original_image in tqdm(original_images):\n",
    "        anchor = preprocess_image(original_image)\n",
    "        image_name = os.path.splitext(os.path.basename(original_image))[0]\n",
    "        augmented_image_folder = os.path.join(image_folder_augmented, image_name)\n",
    "\n",
    "        if not os.path.exists(augmented_image_folder):\n",
    "            print(f\"No augmented image folder found for {original_image}\")\n",
    "            continue  # Skip if no augmented folder\n",
    "\n",
    "        # List all augmented images for this original image\n",
    "        positive_images_paths = [os.path.join(augmented_image_folder, f) for f in os.listdir(augmented_image_folder) if f.endswith('.jpg')]\n",
    "        \n",
    "        if not positive_images_paths:\n",
    "            print(f\"No augmented images found for {original_image}\")\n",
    "            continue  # Skip if no augmented versions\n",
    "\n",
    "        for _ in range(num_triplets_per_anchor):\n",
    "            # Select a random positive sample\n",
    "            positive_image_path = random.choice(positive_images_paths)\n",
    "            positive = preprocess_image(positive_image_path)\n",
    "\n",
    "            # Select a random negative sample (from another original image)\n",
    "            negative_image_path = random.choice([img for img in original_images if img != original_image])\n",
    "            negative = preprocess_image(negative_image_path)\n",
    "\n",
    "            # Append the triplet to the lists\n",
    "            anchor_images.append(anchor)\n",
    "            positive_images.append(positive)\n",
    "            negative_images.append(negative)\n",
    "\n",
    "    return np.array(anchor_images), np.array(positive_images), np.array(negative_images)\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image.\n",
    "    :param image_path: Path to the image.\n",
    "    :return: Preprocessed image array.\n",
    "    \"\"\"\n",
    "    image = load_img(image_path, target_size=(224, 224))  # Resize to match the model input size\n",
    "    image = img_to_array(image)  # Convert to array\n",
    "    image = preprocess_input(image)  # Preprocess for the specific model\n",
    "    return image\n",
    "\n",
    "# Specify the paths to your original and augmented images\n",
    "original_image_folder = './images/original_test/'\n",
    "augmented_image_folder = './images/augmented_test/'\n",
    "\n",
    "# Generate triplets\n",
    "anchor_images, positive_images, negative_images = prepare_triplets(original_image_folder, augmented_image_folder, num_triplets_per_anchor=1)\n",
    "\n",
    "print(f'Generated {len(anchor_images)} triplets for training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    \"\"\"\n",
    "    Base network to be shared (Siamese network)\n",
    "    \"\"\"\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)  # Embedding size is 256\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    y_true is not used.\n",
    "    y_pred contains the anchor, positive and negative embeddings concatenated.\n",
    "    \"\"\"\n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "\n",
    "    # Split the encoding into anchor, positive and negative encodings\n",
    "    anchor = y_pred[:, 0:int(total_lenght*1/3)]\n",
    "    positive = y_pred[:, int(total_lenght*1/3):int(total_lenght*2/3)]\n",
    "    negative = y_pred[:, int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "\n",
    "    # Triplet Loss Formula\n",
    "    pos_dist = K.sum(K.square(anchor - positive), axis=1)\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = K.maximum(basic_loss, 0.0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "base_network = create_base_network(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_input = Input(input_shape, name=\"anchor_input\")\n",
    "positive_input = Input(input_shape, name=\"positive_input\")\n",
    "negative_input = Input(input_shape, name=\"negative_input\")\n",
    "\n",
    "# Generate embeddings for each input\n",
    "encoded_anchor = base_network(anchor_input)\n",
    "encoded_positive = base_network(positive_input)\n",
    "encoded_negative = base_network(negative_input)\n",
    "\n",
    "# Concatenate all embeddings into one vector for loss calculation\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=1)\n",
    "\n",
    "# Define the model with anchor, positive, and negative inputs\n",
    "triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged_vector)\n",
    "\n",
    "# Compile the model with the triplet loss\n",
    "triplet_model.compile(loss=triplet_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor images shape: (1000, 224, 224, 3)\n",
      "Positive images shape: (1000, 224, 224, 3)\n",
      "Negative images shape: (1000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'Anchor images shape: {anchor_images.shape}')\n",
    "print(f'Positive images shape: {positive_images.shape}')\n",
    "print(f'Negative images shape: {negative_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 24s/step - loss: 441588.3125\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m785s\u001b[0m 24s/step - loss: 603.6125\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m787s\u001b[0m 25s/step - loss: 224.2026\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m790s\u001b[0m 25s/step - loss: 1241.5020\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 25s/step - loss: 155.9232\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 25s/step - loss: 2.4176\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m886s\u001b[0m 28s/step - loss: 0.5998\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m812s\u001b[0m 25s/step - loss: 0.2240\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m796s\u001b[0m 25s/step - loss: 0.0631\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 25s/step - loss: 0.0500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19455f77e30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_model.fit(\n",
    "    [anchor_images, positive_images, negative_images],\n",
    "    np.zeros((anchor_images.shape[0], 1)),  # Dummy labels since the loss is custom\n",
    "    batch_size=32,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding model using the trained base network\n",
    "embedding_model = Model(inputs=base_network.input, outputs=base_network.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.save('./models/embedding_model_01.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embedding_model = load_model('./models/embedding_model_01.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for the embedding model.\n",
    "    :param image_path: Path to the image.\n",
    "    :return: Preprocessed image array.\n",
    "    \"\"\"\n",
    "    image = load_img(image_path, target_size=(224, 224))  # Resize to match the model input size\n",
    "    image = img_to_array(image)  # Convert to array\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    image = preprocess_input(image)  # Preprocess for VGG16 model\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b241f8deb0984855a0837c4a91eaee71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Folder containing original images\n",
    "original_image_folder = './images/original_test/'\n",
    "\n",
    "# Dictionary to store image embeddings\n",
    "embeddings_dict = {}\n",
    "\n",
    "# List all original images\n",
    "original_images = [os.path.join(original_image_folder, f) for f in os.listdir(original_image_folder) if f.endswith('.jpg')]\n",
    "\n",
    "counter = 0\n",
    "for image_path in tqdm(original_images):\n",
    "    # if(counter < 100):\n",
    "    # Preprocess the image\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    \n",
    "    # Compute the embedding\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        embedding = embedding_model.predict(preprocessed_image)\n",
    "        embedding = embedding.flatten()\n",
    "    \n",
    "    # Store the embedding in the dictionary with the image path as the key\n",
    "    embeddings_dict[image_path] = embedding\n",
    "    counter += 1\n",
    "\n",
    "np.savez('./embeddings_augmented/embeddings_dict_03.npz', **embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('./embeddings_augmented/embeddings_dict_01.npz', **embeddings_dict)\n",
    "\n",
    "# load again by doing:\n",
    "# loaded = np.load('embeddings_dict.npz')\n",
    "# Convert the loaded data back to a dictionary\n",
    "# embeddings_dict = {key: loaded[key] for key in loaded}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
      "The most similar image is ./images/original_test/Cuba_66467_EWO-s.jpg with a distance of 0.0\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar_image(new_image_embedding, embeddings_dict):\n",
    "    min_distance = float('inf')\n",
    "    most_similar_image = None\n",
    "\n",
    "    # Ensure new_image_embedding is 1D\n",
    "    new_image_embedding = np.asarray(new_image_embedding).flatten()\n",
    "\n",
    "    for image_path, embedding in embeddings_dict.items():\n",
    "        # Ensure each embedding is 1D\n",
    "        embedding = np.asarray(embedding).flatten()\n",
    "\n",
    "        # Calculate the cosine distance\n",
    "        distance = cosine(new_image_embedding, embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            most_similar_image = image_path\n",
    "\n",
    "    return most_similar_image, min_distance\n",
    "\n",
    "# Example usage: Compute embedding for a new image\n",
    "new_image_path = './images/augmented_test/Aaland_181874_MI-s/Aaland_181874_MI-s_contrast.jpg'\n",
    "new_image_preprocessed = preprocess_image(new_image_path)\n",
    "\n",
    "# Ensure new_image_preprocessed is 4D (batch size, height, width, channels)\n",
    "# new_image_preprocessed = np.expand_dims(new_image_preprocessed, axis=0)\n",
    "\n",
    "new_image_embedding = embedding_model.predict(new_image_preprocessed)\n",
    "new_image_embedding = new_image_embedding.flatten()\n",
    "\n",
    "# Find the most similar image\n",
    "most_similar_image, distance = find_most_similar_image(new_image_embedding, embeddings_dict)\n",
    "print(f'The most similar image is {most_similar_image} with a distance of {distance}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
